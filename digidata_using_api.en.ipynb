{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "fi"
   },
   "source": [
    "# National Library ALTO XML Text Mining Examples (Digitalia-Notebook)\n",
    "\n",
    "Version history\n",
    "* 0.2  30.9.2021 Downloading data using api. Fixes to code and translations (Erno Liukkonen)\n",
    "* 0.1 23.4.2019 Small text corrections\n",
    "* 0.02 Digitalia-project, minor fixes\n",
    "* 0.01 15.1.2019 (Mika Koistinen)\n",
    "\n",
    "\n",
    "# Thanks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<table><tr><td>\n",
    "<img src=\"https://blogs.helsinki.fi/digitalia/files/2018/10/sosiaali_fi_90p.jpg\" style=\"height:100px;width:100%\">\n",
    "    </td><td>\n",
    "<img src=\"https://blogs.helsinki.fi/digitalia/files/2018/10/fi_EU_rgb_90p.jpg\" style=\"height:100px;width:100%\">\n",
    "    </td><td>\n",
    "<img src=\"https://blogs.helsinki.fi/digitalia/files/2015/10/digitalia_pien_512.png\" style=\"height:70px;width:100%\">\n",
    "    </td></td></tr></table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "fi"
   },
   "source": [
    "# Downloading data using application programming interface (API)\n",
    "\n",
    "API is used to download data. Search collections using https://digi.kansalliskirjasto.fi/search and copy results page address to searchResultsUrl variable that is below (default value is https://digi.kansalliskirjasto.fi/search?startDate=1870-10-01&endDate=1870-12-31&pages=1-50&title=1457-4721&formats=NEWSPAPER).\n",
    "\n",
    "bindingSearchQuery function downloads information related to search results using API. Search results page address is given to function as a parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib\n",
    "from urllib.error import URLError, HTTPError\n",
    "from socket import timeout\n",
    "import simplejson as json\n",
    "\n",
    "searchResultsUrl = \"https://digi.kansalliskirjasto.fi/search?startDate=1870-10-01&endDate=1870-12-31&pages=1-50&title=1457-4721&formats=NEWSPAPER\"\n",
    "\n",
    "request_headers = {\n",
    "\"User-Agent\": \"Notebook dam-rajapinta\",\n",
    "\"Referer\": \"Notebook dam-rajapinta\",\n",
    "\"Connection\": \"keep-alive\" \n",
    "}\n",
    "\n",
    "pageImageTemplate = \"\"\n",
    "altoXmlTemplate = \"\"\n",
    "altoTxtTemplate = \"\"\n",
    "\n",
    "def bindingSearchQuery(digiResultsUrl):\n",
    "    \n",
    "    \n",
    "    global pageImageTemplate\n",
    "    global altoXmlTemplate\n",
    "    global altoTxtTemplate\n",
    "\n",
    "    parameters = digiResultsUrl[digiResultsUrl.index('?'):]\n",
    "\n",
    "    currentRows = []\n",
    "    result = \"\"\n",
    "    noMoreResults = False\n",
    "    isFirstSearch = True\n",
    "\n",
    "    digiBindingSearchURL = 'https://digi.kansalliskirjasto.fi/api/dam/binding-search' + parameters\n",
    "\n",
    "    while noMoreResults == False:\n",
    "    \n",
    "      req = urllib.request.Request(digiBindingSearchURL, headers=request_headers)\n",
    "      try:\n",
    "        response = urllib.request.urlopen(req, timeout=30)\n",
    "        responseResult = response.read()\n",
    "        result = json.loads(responseResult)\n",
    "\n",
    "        pageImageTemplate = result[\"pageImageTemplate\"]\n",
    "        altoXmlTemplate = result[\"altoXmlTemplate\"]\n",
    "        altoTxtTemplate = result[\"altoTxtTemplate\"]\n",
    "\n",
    "        if len(result[\"rows\"]) != 0:\n",
    "          currentRows = currentRows + result[\"rows\"]\n",
    "        else:\n",
    "          noMoreResults = True\n",
    "\n",
    "        if isFirstSearch == True:\n",
    "          digiBindingSearchURL = 'https://digi.kansalliskirjasto.fi/api/dam/binding-search/' + result[\"scrollId\"]\n",
    "          isFirstSearch = False\n",
    "\n",
    "      except HTTPError as e:\n",
    "        content = e.read()\n",
    "      except ConnectionError as e:\n",
    "        print(\"No connection to the server!\")\n",
    "      except URLError as e:\n",
    "        print(\"No connection to the server!\")\n",
    "      except TimeoutError as e:\n",
    "        print(\"No connection to the server!\")\n",
    "      except timeout:\n",
    "        print(\"No connection to the server!\")\n",
    "\n",
    "    result[\"rows\"] = currentRows\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next ALTO- and text-files are downloaded using urlretrieve function, function calls are build using information that was retrieved using bindSearchQuery function. Files are downloaded to the \"downloads\"-folder, which is in the same folder where the notebook was started."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import urllib\n",
    "from urllib.error import URLError, HTTPError\n",
    "from socket import timeout\n",
    "\n",
    "def urlretrieve(url, localfile):\n",
    "    \n",
    "    global searchResultsUrl\n",
    "    \n",
    "    try:\n",
    "      req = urllib.request.Request(url, headers=request_headers)\n",
    "      f = urllib.request.urlopen(req, timeout=30)\n",
    "\n",
    "      # Open our local file for writing\n",
    "      with open(localfile, \"wb\") as fl:\n",
    "          fl.write(f.read())\n",
    "\n",
    "    #handle errors\n",
    "    except HTTPError as e:\n",
    "      print(\"No connection to the server!\")\n",
    "      return 0 \n",
    "    except URLError as e:\n",
    "      print(\"No connection to the server!\")\n",
    "      return 0\n",
    "    except TimeoutError as e:\n",
    "      print(\"No connection to the server!\")\n",
    "      return 0\n",
    "    except timeout:\n",
    "      print(\"No connection to the server!\")\n",
    "      return 0\n",
    "\n",
    "    return 1\n",
    "\n",
    "notebookPath =  os.path.dirname(os.path.realpath(\"__file__\"))\n",
    "\n",
    "result = bindingSearchQuery(searchResultsUrl)\n",
    "\n",
    "rows = result[\"rows\"]\n",
    "\n",
    "isFirstResultRow = True\n",
    "firstResultAlto = \"\"\n",
    "firstResultPath = \"\"\n",
    " \n",
    "resultCounter = 1\n",
    "\n",
    "for row in rows:\n",
    "    bindingTitle = row[\"bindingTitle\"]\n",
    "    bindingId = str(row[\"bindingId\"])\n",
    "    pageNumber =  str(row[\"pageNumber\"])\n",
    "    baseUrl = row[\"baseUrl\"]\n",
    "    \n",
    "    altoUrl  = baseUrl + altoXmlTemplate.replace(\"{{page}}\", str(pageNumber))\n",
    "    txtUrl  = baseUrl + altoTxtTemplate.replace(\"{{page}}\", str(pageNumber))\n",
    "    \n",
    "    altoPath = notebookPath + \"/downloads/\" + bindingTitle + \"_\" + bindingId + \"/alto/\"\n",
    "    txtPath = notebookPath + \"/downloads/\" + bindingTitle + \"_\" + bindingId + \"/txt/\"\n",
    "\n",
    "    if not os.path.exists(altoPath):\n",
    "        os.makedirs(altoPath)\n",
    "    \n",
    "    if not os.path.exists(txtPath):\n",
    "        os.makedirs(txtPath)\n",
    "        \n",
    "    downloadPathAlto = altoPath + bindingTitle + \"_\" + bindingId + \"_page_\" + pageNumber + \".xml\"\n",
    "    downloadPathTxt = txtPath + bindingTitle + \"_\" + bindingId + \"_page_\" + pageNumber + \".txt\"\n",
    "    \n",
    "    print(\"Downloading result: \" + str(resultCounter) + \"/\" + str(len(rows)))\n",
    "    \n",
    "    urlretrieve(altoUrl, downloadPathAlto)\n",
    "    urlretrieve(txtUrl, downloadPathTxt)\n",
    "    \n",
    "    resultCounter += 1\n",
    "    \n",
    "    if isFirstResultRow:\n",
    "        firstResultAlto = downloadPathAlto\n",
    "        firstResultPath = altoPath\n",
    "        isFirstResultRow = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load single file to DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "global firstResultPath\n",
    "\n",
    "#ALTO folder path of first binding that was was downloaded using API.\n",
    "path = firstResultPath \n",
    "\n",
    "print(path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#you can check what files are in the given path:\n",
    "sorted(os.listdir(path)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup as bs\n",
    "\n",
    "global firstResultAlto\n",
    "\n",
    "#sets the path of ALTO file that was first downloaded to filename variable.\n",
    "filename = firstResultAlto \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xmlsoup=bs(open(filename,\"r\"),\"lxml\")\n",
    "xmldata=str(xmlsoup)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "import csv\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#modfies XML-structure to  pandas.DataFrame\n",
    "#original function can be found from:\n",
    "#http://www.austintaylor.io/lxml/python/pandas/xml/dataframe/2016/07/08/convert-xml-to-pandas-dataframe/\n",
    "class XML2DataFrame:\n",
    "\n",
    "    def __init__(self, xml_data):\n",
    "        self.root = ET.XML(xml_data)\n",
    "\n",
    "    def parse_root(self, root):\n",
    "        return [self.parse_element(child) for child in iter(root)]\n",
    "\n",
    "    def parse_element(self, element, parsed=None):\n",
    "        if parsed is None:\n",
    "            parsed = dict()\n",
    "        for key in element.keys():\n",
    "            parsed[key] = element.attrib.get(key)\n",
    "        if element.text:\n",
    "            parsed[element.tag] = element.text\n",
    "        for child in list(element):\n",
    "            self.parse_element(child, parsed)\n",
    "        return parsed\n",
    "\n",
    "    def process_data(self):\n",
    "        structure_data = self.parse_root(self.root)\n",
    "        return pd.DataFrame(structure_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creates data structure\n",
    "xml2df = XML2DataFrame(xmldata)\n",
    "xml_dataframe = xml2df.process_data()\n",
    "#shows data structures columns\n",
    "xml_dataframe.columns\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creates pandas.DataFrame from files texts\n",
    "strings=xmlsoup.find_all([\"string\",\"hyp\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creates xmldata2, where strings are separately inside structure\n",
    "xmldata2='<?xml version=\"1.0\" encoding=\"UTF-8\"?><html><body>'\n",
    "for s in strings:\n",
    "    xmldata2=xmldata2+str(s)+\"\\n\"\n",
    "xmldata2=xmldata2+'</body></html>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xmldata2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree = ET.XML(xmldata2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#creates pd.Dataframe and prints it\n",
    "data = []\n",
    "for el in tree.iterfind('./*'):\n",
    "    for i in el.iterfind('*'):\n",
    "        data.append(dict(i.items()))\n",
    "df = pd.DataFrame(data)        \n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#prints full text\n",
    "contents=\"\"\n",
    "for w in df['content']: \n",
    "   #if w!=\"-\":        \n",
    "   contents=contents+w+\" \"\n",
    "   #cont=contents.replace(\"\\n\",\" \")\n",
    "contents2=contents.replace(\" - \",\"\")\n",
    "print(contents2)     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#creates word tokens\n",
    "#NLTK\n",
    "#pip install nltk\n",
    "import nltk \n",
    "tokens=nltk.word_tokenize(contents2)\n",
    "tokens[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#removing stopwords\n",
    "from nltk.corpus import stopwords\n",
    "import io\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "#stop words\n",
    "stop_words=set(stopwords.words('finnish'))\n",
    "#print(stopwords[0:10],len(stopwords))\n",
    "#filtered_sentence = [w for w in content if not w in stop_words]\n",
    "\n",
    "#clears stop words\n",
    "filtered_content=[]\n",
    "for r in tokens:\n",
    "    #print(r)\n",
    "    if not r in stop_words:\n",
    "        filtered_content.append(r)\n",
    "\n",
    "#stop words, under 1 character words and over 20 character words are removed\n",
    "filtered_content2=[]\n",
    "for r in tokens:\n",
    "    #print(r)\n",
    "    if not r in stop_words and len(r) <20 and len(r)>1: # and\n",
    "        filtered_content2.append(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prints different word tokens\n",
    "freqz=nltk.FreqDist(tokens)\n",
    "freqz2=nltk.FreqDist(filtered_content)\n",
    "freqz3=nltk.FreqDist(filtered_content2)\n",
    "\n",
    "print(\"full text\")\n",
    "print(freqz.most_common()[0:15])\n",
    "print(\"stopwords removed\")\n",
    "print(freqz2.most_common()[0:15])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WordClouds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Example can be found here:\n",
    "#https://github.com/amueller/word_cloud\n",
    "#!pip install wordcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "tokens_str = ''.join(tokens)\n",
    "fig = plt.figure(figsize=(24, 14)) \n",
    "wordcloud=WordCloud().generate(tokens_str)\n",
    "#wordcloud = WordCloud(max_font_size=50,min_font_size=5).generate(content_str)\n",
    "plt.subplot(221)\n",
    "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"1 document (all tokens)\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "fig = plt.figure(figsize=(24, 14)) \n",
    "content_str2 = ''.join(filtered_content)\n",
    "wordcloud2=WordCloud().generate(content_str2)\n",
    "#wordcloud2 = WordCloud(max_font_size=40,min_font_size=5).generate(content_str2)\n",
    "plt.subplot(222)\n",
    "plt.imshow(wordcloud2, interpolation=\"bilinear\")\n",
    "plt.title(\"1 document (removed stopwords)\")\n",
    "\n",
    "\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "print(\"words count:\",len(tokens_str),len(content_str2))#,len(content_str3))\n",
    "#content_str2\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multiple files from folders\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading all XML files from the folders\n",
    "import os\n",
    "global firstResultPath\n",
    "\n",
    "#ALTO folder path of first binding that was downloaded using API.\n",
    "path = firstResultPath \n",
    "files=os.listdir(path)\n",
    "#sorted(os.listdir(path))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!this can take some time\n",
    "strings2=[]\n",
    "for file1 in files:\n",
    "    filename=path+file1\n",
    "    xmlsoup=bs(open(filename,\"r\"),\"lxml\")\n",
    "    strings2.extend(xmlsoup.find_all(\"string\"))\n",
    "\n",
    "print(\"words in all pages in the folder:\",len(strings2))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "content22=[stri2['content'].lower() for stri2 in strings2[0:]]\n",
    "len(content22)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#removes words under 2 characters\n",
    "content2=[stri2 for stri2 in content22 if len(stri2)>1]\n",
    "len(content2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#removes stopwords\n",
    "filtered_content3=[]\n",
    "for r in content2:\n",
    "    #print(r)\n",
    "    if not r in stop_words:\n",
    "        filtered_content2.append(r)\n",
    "len(filtered_content2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freqz3=nltk.FreqDist(content2)\n",
    "freqz3.most_common()[0:15]\n",
    "\n",
    "freqz4=nltk.FreqDist(filtered_content2)\n",
    "\n",
    "print(freqz3.most_common()[0:10])\n",
    "print(freqz4.most_common()[0:10])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WordClouds (all pages in a folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "content_str2 = ' '.join(content2)\n",
    "\n",
    "\n",
    "fig = plt.figure(figsize=(16, 10))\n",
    "plt.subplot(223)\n",
    "wordcloud2 = WordCloud(max_font_size=50,min_font_size=5).generate(content_str2)\n",
    "\n",
    "plt.imshow(wordcloud2, interpolation=\"bilinear\")\n",
    "plt.title(\"all \"+str(len(files))+\" documents, and \"+str(len(content2))+\" words, from folder\")\n",
    "\n",
    "plt.axis(\"off\")\n",
    "\n",
    "\n",
    "content_str3 = ' '.join(filtered_content2)\n",
    "\n",
    "plt.subplot(224)\n",
    "wordcloud3 = WordCloud(max_font_size=50,min_font_size=5).generate(content_str3)\n",
    "plt.imshow(wordcloud3, interpolation=\"bilinear\")\n",
    "\n",
    "plt.title(\"all \"+str(len(files))+\" documents, and \"+str(len(filtered_content2))+\" words, from folder (removed stopwords)\")\n",
    "\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Topic Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#based on \n",
    "#https://www.analyticsvidhya.com/blog/2016/08/beginners-guide-to-topic-modeling-in-python/\n",
    "#read all docs into the corpora\n",
    "import os\n",
    "files=os.listdir(path)\n",
    "FILES=[]\n",
    "doc_complete=[]\n",
    "for ind in range(0,len(files)):\n",
    "    filename=path+files[ind]\n",
    "    FILES.append(filename)\n",
    "    xmlsoup=bs(open(filename,\"r\"),\"xml\")\n",
    "    strings=xmlsoup.find_all(\"String\")\n",
    "    strings2=[]\n",
    "    for str1 in strings:\n",
    "        strings2.append(str1['CONTENT'].lower())\n",
    "    doc_complete.append(strings2)\n",
    "\n",
    "len(doc_complete),len(content2),len(filtered_content2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "import string\n",
    "stop = set(stopwords.words('finnish'))\n",
    "exclude = set(string.punctuation)\n",
    "lemma = WordNetLemmatizer()\n",
    "\n",
    "def clean(doc):\n",
    "    doc2=[word for word in doc if len(word)>3]       \n",
    "    stop_free = \" \".join([i for i in doc2 if i not in stop])\n",
    "    punc_free = ''.join(ch for ch in stop_free if ch not in exclude)\n",
    "    normalized = \" \".join(lemma.lemmatize(word) for word in punc_free.split())\n",
    "    return normalized\n",
    "\n",
    "    \n",
    "doc_clean = [clean(doc).split() for doc in doc_complete]        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#doc_clean=docs2#filtered_content2#map(unicode,filtered_content2)\n",
    "# Importing Gensim\n",
    "import gensim\n",
    "from gensim import corpora\n",
    "dictionary = corpora.Dictionary(doc_clean )\n",
    "#dictionary\n",
    "# Creating the term dictionary of our courpus, where every unique term is assigned an index. dictionary = corpora.Dictionary(doc_clean)\n",
    "\n",
    "# Converting list of documents (corpus) into Document Term Matrix using dictionary prepared above.\n",
    "doc_term_matrix = [dictionary.doc2bow(doc) for doc in doc_clean]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the object for LDA model using gensim library\n",
    "\n",
    "Lda = gensim.models.ldamodel.LdaModel\n",
    "\n",
    "# Running and Training LDA model on the document term matrix.\n",
    "ldamodel = Lda(doc_term_matrix, num_topics=100, id2word = dictionary, passes=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Results\n",
    "print(ldamodel.print_topics(num_topics=100, num_words=3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ldamodel.print_topics(num_topics=10, num_words=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(doc_complete),len(doc_clean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TOPIC MODELING2 https://de.dariah.eu/tatom/topic_model_python.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FILES\n",
    "FILES2=[]\n",
    "import os\n",
    "global firstResultPath\n",
    "\n",
    "#ALTO folder path of first binding that was was downloaded using API, changes \"alto\" from the path to \"txt\". \n",
    "path = firstResultPath.replace(\"alto\",\"txt\") \n",
    "for file in os.listdir(path):\n",
    "    if file.endswith(\".txt\"):\n",
    "        FILES2.append(\"\".join([path,file]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############\n",
    "\n",
    "import numpy as np\n",
    "#FILES[0:4]\n",
    "stop = list(set(stopwords.words('finnish')))\n",
    "\n",
    "import sklearn.feature_extraction.text as text\n",
    "vectorizer = text.CountVectorizer(input='filename', stop_words=stop, min_df=1)\n",
    "dtm = vectorizer.fit_transform(FILES2).toarray()\n",
    "vocab = np.array(vectorizer.get_feature_names())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtm.shape,len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import decomposition\n",
    "num_topics = 10\n",
    "num_top_words = 10\n",
    "clf = decomposition.NMF(n_components=num_topics, random_state=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doctopic = clf.fit_transform(dtm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_words = []\n",
    "for topic in clf.components_:\n",
    "   word_idx = np.argsort(topic)[::-1][0:num_top_words]\n",
    "   topic_words.append([vocab[i] for i in word_idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doctopic = doctopic / np.sum(doctopic, axis=1, keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "novel_names = []\n",
    "\n",
    "# turn this into an array so we can use NumPy functions\n",
    "#novel_names = np.asarray(novel_names)\n",
    "novel_names=np.asarray(FILES2)\n",
    "doctopic_orig = doctopic.copy()\n",
    "\n",
    "# use method described in preprocessing section\n",
    "num_groups = len(set(novel_names))\n",
    "doctopic_grouped = np.zeros((num_groups, num_topics))\n",
    "\n",
    "for i, name in enumerate(sorted(set(novel_names))):\n",
    "        doctopic_grouped[i, :] = np.mean(doctopic[novel_names == name, :], axis=0)\n",
    "\n",
    "doctopic = doctopic_grouped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "novels = sorted(set(novel_names))\n",
    "\n",
    "print(\"Top NMF topics in...\")\n",
    "\n",
    "\n",
    "for i in range(len(doctopic)):\n",
    "        top_topics = np.argsort(doctopic[i,:])[::-1][0:3]\n",
    "        top_topics_str = ' '.join(str(t) for t in top_topics)\n",
    "        print(\"{}: {}\".format(novels[i], top_topics_str))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for t in range(len(topic_words)):\n",
    "   print(\"Topic {}: {}\".format(t, ' '.join(topic_words[t][:15])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "nbTranslate": {
   "displayLangs": [
    "fi",
    "en"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "fi",
   "targetLang": "en",
   "useGoogleTranslate": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
