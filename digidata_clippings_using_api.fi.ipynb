{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "fi"
   },
   "source": [
    "# Kansalliskirjaston leikkeiden tekstinlouhintaesimerkkejä (Digitalia-notebook)\n",
    "\n",
    "Versiohistoria\n",
    "* 0.3  7.2.2024 Leikkeiden lataaminen rajapinnan kautta, koodiin tarvittavia muutoksia (Erno Liukkonen)\n",
    "* 0.2  30.9.2021 Aineiston lataaminen rajapinnan kautta, korjauksia koodiin ja käännöksiin (Erno Liukkonen) \n",
    "* 0.1  23.4.2019 Pieniä tekstikorjauksia\n",
    "* 0.02 Digitalia-project, pieniä korjauksia\n",
    "* 0.01 15.1.2019 (Mika Koistinen)\n",
    "\n",
    "\n",
    "# Kiitokset\n",
    "\n",
    "\n",
    "\n",
    "<table><tr><td>\n",
    "<img src=\"https://blogs.helsinki.fi/digitalia/files/2018/10/sosiaali_fi_90p.jpg\" style=\"height:100px;width:100%\">\n",
    "    </td><td>\n",
    "<img src=\"https://blogs.helsinki.fi/digitalia/files/2018/10/fi_EU_rgb_90p.jpg\" style=\"height:100px;width:100%\">\n",
    "    </td><td>\n",
    "<img src=\"https://blogs.helsinki.fi/digitalia/files/2015/10/digitalia_pien_512.png\" style=\"height:70px;width:100%\">\n",
    "    </td></td></tr></table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "fi"
   },
   "source": [
    "# Leikkeiden lataaminen rajapinnan kautta\n",
    "\n",
    "Leikkeiden lataaminen tapahtuu rajapintaa hyödyntäen. Suorita leikkeiden haku https://digi.kansalliskirjasto.fi/clippings osoitteessa ja kopioi hakutulokset sisältävän sivun osoite alla olevaan searchResultsUrl muuttujaan (oletuksena nyt https://digi.kansalliskirjasto.fi/clippings?formats=NEWSPAPER&startDate=1890-08-01&endDate=1890-09-01&title=1458-2457&resultMode=THUMB).\n",
    "\n",
    "clippingsSearchQuery funktio suorittaa haettujen leikkeiden tietojen lataamisen rajapinnan kautta. Funktiolle annetaan parametrinä hakutulokset sisältävän sivun osoite."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import urllib\n",
    "from urllib.error import URLError, HTTPError\n",
    "from socket import timeout\n",
    "import simplejson as json\n",
    "\n",
    "searchResultsUrl = \"https://digi.kansalliskirjasto.fi/clippings?formats=NEWSPAPER&startDate=1890-08-01&endDate=1890-09-01&title=1458-2457&resultMode=THUMB\"\n",
    "\n",
    "request_headers = {\n",
    "\"User-Agent\": \"Notebook dam-rajapinta\",\n",
    "\"Referer\": \"Notebook dam-rajapinta\",\n",
    "\"Connection\": \"keep-alive\" \n",
    "}\n",
    "\n",
    "def clippingsSearchQuery(digiResultsUrl):\n",
    "\n",
    "    parameters = digiResultsUrl[digiResultsUrl.index('?'):]\n",
    "\n",
    "    currentRows = []\n",
    "    result = \"\"\n",
    "    noMoreResults = False\n",
    "    isFirstSearch = True\n",
    "\n",
    "    digiClippingsSearchURL = 'https://digi.kansalliskirjasto.fi/api/dam/clippings-search/' + parameters\n",
    "\n",
    "    while noMoreResults == False:\n",
    "    \n",
    "      req = urllib.request.Request(digiClippingsSearchURL, headers=request_headers)\n",
    "      try:\n",
    "        response = urllib.request.urlopen(req, timeout=30)\n",
    "        responseResult = response.read()\n",
    "        result = json.loads(responseResult)\n",
    "\n",
    "        if len(result[\"rows\"]) != 0:\n",
    "          currentRows = currentRows + result[\"rows\"]\n",
    "        else:\n",
    "          noMoreResults = True\n",
    "\n",
    "        if isFirstSearch == True:\n",
    "          digiClippingsSearchURL = 'https://digi.kansalliskirjasto.fi/api/dam/clippings-search/' + result[\"scrollId\"]\n",
    "          isFirstSearch = False\n",
    "\n",
    "      except HTTPError as e:\n",
    "        content = e.read()\n",
    "      except ConnectionError as e:\n",
    "        print(\"Ei yhteyttä palvelimeen!\")\n",
    "      except URLError as e:\n",
    "        print(\"Ei yhteyttä palvelimeen!\")\n",
    "      except TimeoutError as e:\n",
    "        print(\"Ei yhteyttä palvelimeen!\")\n",
    "      except timeout:\n",
    "        print(\"Ei yhteyttä palvelimeen!\")\n",
    "\n",
    "    result[\"rows\"] = currentRows\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seuraavaksi suoritetaan varsinainen leikkeiden teksti- ja kuvatiedostojen lataus urlretrieve funktion avulla. Tiedostojen latauksissa tarvittavien tietojen muodostamisessa käytetään clippingsSearchQuery funktion kautta saatuja tietoja. Tiedostot ladataan oletuksena downloads-hakemistoon, joka sijaitsee samassa hakemistossa mistä notebook käynnistettiin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import urllib\n",
    "import simplejson as json\n",
    "\n",
    "def urlretrieve(url, localfile):\n",
    "\n",
    "    try:\n",
    "        req = urllib.request.Request(url, headers=request_headers)\n",
    "        response = urllib.request.urlopen(req, timeout=30)\n",
    "        responseResult = response.read()\n",
    "\n",
    "        if localfile.endswith(\".txt\"):\n",
    "            tContent = json.loads(responseResult)\n",
    "            with open(localfile, \"w\", encoding=\"utf-8\") as fl:\n",
    "              fl.write(' '.join(tContent))\n",
    "        else:\n",
    "            with open(localfile, \"wb\") as fl:\n",
    "              fl.write(responseResult)\n",
    "\n",
    "    #handle errors\n",
    "    except Exception as e:\n",
    "      print(\"Ei yhteyttä palvelimeen!\") + \" \" + str(e)\n",
    "      return 0\n",
    "\n",
    "    if not os.path.exists(localfile):\n",
    "      return 0\n",
    "\n",
    "    return 1\n",
    "\n",
    "notebookPath =  os.path.dirname(os.path.realpath(\"__file__\"))\n",
    "\n",
    "result = clippingsSearchQuery(searchResultsUrl)\n",
    "\n",
    "rows = result[\"rows\"]\n",
    "\n",
    "isFirstResultRow = True\n",
    "firstResultTxt = \"\"\n",
    "firstResultPath = \"\"\n",
    " \n",
    "resultCounter = 1\n",
    "\n",
    "for row in rows:\n",
    "    data = row[\"data\"] \n",
    "    bindingTitle = data[\"bindingTitle\"]\n",
    "    bindingId = str(data[\"bindingId\"])\n",
    "    articleId = str(data[\"articleId\"])\n",
    "    \n",
    "    ocrTextUrl = row[\"ocrTextUrl\"]\n",
    "    currentImageUrls = []\n",
    "    currentArticleRegionIds = row[\"articleRegionIds\"].split(',')\n",
    "    currentPageNumbers = row[\"articleRegionPageNumbers\"].split(',')\n",
    "\n",
    "    if \"imageUrlTemplate\" in row and row[\"imageUrlTemplate\"] != None:\n",
    "          if \"articleRegionIds\" in row and row[\"articleRegionIds\"] != None:\n",
    "            for regionId in row[\"articleRegionIds\"].split(','):\n",
    "                currentImageUrls.append(row[\"imageUrlTemplate\"].replace(\"{{regionId}}\", regionId))\n",
    "    \n",
    "    txtPath = notebookPath + \"/downloads/\" + bindingTitle + \"_\" + bindingId + \"_\" + articleId + \"/txt/\"\n",
    "    imgPath = notebookPath + \"/downloads/\" + bindingTitle + \"_\" + bindingId + \"_\" + articleId +\"/jpg/\"\n",
    "\n",
    "    if not os.path.exists(txtPath):\n",
    "        os.makedirs(txtPath)\n",
    "    \n",
    "    if not os.path.exists(imgPath):\n",
    "        os.makedirs(imgPath)\n",
    "        \n",
    "    downloadPathTxt = txtPath + bindingTitle + \"_\" + bindingId + \"_\" + articleId + \"_page_\" + currentPageNumbers[0] + \".txt\"\n",
    "    \n",
    "    print(\"Ladataan tulosta: \" + str(resultCounter) + \"/\" + str(len(rows)))\n",
    "    \n",
    "    urlretrieve(ocrTextUrl, downloadPathTxt)\n",
    "    \n",
    "    for index, currentImageUrl in enumerate(currentImageUrls):\n",
    "        downloadPathJpg = imgPath + bindingTitle + \"_\" + bindingId + \"_\" + articleId + \"_\" + currentArticleRegionIds[index] + \"_page_\" + currentPageNumbers[index] + \".jpg\"\n",
    "        urlretrieve(currentImageUrl, downloadPathJpg)\n",
    "    \n",
    "    resultCounter += 1\n",
    "    \n",
    "    if isFirstResultRow:\n",
    "        firstResultTxt = downloadPathTxt\n",
    "        firstResultPath = txtPath\n",
    "        isFirstResultRow = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Luetaan yksi ladatuista leikkeiden tekstitiedostoista"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "global firstResultPath\n",
    "\n",
    "#Ensimmäisen ladatun leikkeen tekstitiedoston polku, joka rajapinnan kautta ladattiin\n",
    "path = firstResultPath \n",
    "\n",
    "print(path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tällä voi katsoa mitä tiedostoja kansiossa:\n",
    "sorted(os.listdir(path)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup as bs\n",
    "\n",
    "global firstResultTxt\n",
    "\n",
    "#luetaan ensimmäisen ladatun leikkeen teksti\n",
    "filename = firstResultTxt\n",
    "with open(firstResultTxt, 'r', encoding=\"utf-8\") as f:\n",
    "        content = f.read()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#luodaan sanatokenit\n",
    "#NLTK\n",
    "#pip install nltk\n",
    "import nltk \n",
    "tokens=nltk.word_tokenize(content)\n",
    "tokens[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Frequencies /sanojen esiintymistiheydet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#removing stopwords\n",
    "from nltk.corpus import stopwords\n",
    "import io\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "#sulkusanat\n",
    "stop_words=set(stopwords.words('finnish'))\n",
    "#print(stopwords[0:10],len(stopwords))\n",
    "#filtered_sentence = [w for w in content if not w in stop_words]\n",
    "\n",
    "#sulkusanat poistetaan\n",
    "filtered_content=[]\n",
    "for r in tokens:\n",
    "    #print(r)\n",
    "    if not r in stop_words:\n",
    "        filtered_content.append(r)\n",
    "\n",
    "#sulkusanat alle 1 merkin sanat ja yli 20 merkin sanat poistetaan\n",
    "filtered_content2=[]\n",
    "for r in tokens:\n",
    "    #print(r)\n",
    "    if not r in stop_words and len(r) <20 and len(r)>1: # and\n",
    "        filtered_content2.append(r)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tulostetaan osa eritavoin esikäsitellyistä tokeneista\n",
    "freqz=nltk.FreqDist(tokens)\n",
    "freqz2=nltk.FreqDist(filtered_content)\n",
    "freqz3=nltk.FreqDist(filtered_content2)\n",
    "\n",
    "print(\"koko teksti\")\n",
    "print(freqz.most_common()[0:15])\n",
    "print(\"stopwords poistettu\")\n",
    "print(freqz2.most_common()[0:15])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WordClouds / sanapilvet "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#löytyy täältä esimerkkiä \n",
    "#https://github.com/amueller/word_cloud\n",
    "#!pip install wordcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "tokens_str = ' '.join(tokens)\n",
    "fig = plt.figure(figsize=(24, 14)) \n",
    "wordcloud=WordCloud().generate(tokens_str)\n",
    "#wordcloud = WordCloud(max_font_size=50,min_font_size=5).generate(content_str)\n",
    "plt.subplot(221)\n",
    "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"1 document (all tokens)\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "fig = plt.figure(figsize=(24, 14)) \n",
    "content_str2 = ' '.join(filtered_content)\n",
    "wordcloud2=WordCloud().generate(content_str2)\n",
    "#wordcloud2 = WordCloud(max_font_size=40,min_font_size=5).generate(content_str2)\n",
    "plt.subplot(222)\n",
    "plt.imshow(wordcloud2, interpolation=\"bilinear\")\n",
    "plt.title(\"1 document (removed stopwords)\")\n",
    "\n",
    "\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "print(\"sanojen määrä:\",len(tokens),len(filtered_content))#,len(content_str3))\n",
    "#content_str2\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Useampi tiedosto kansiosta\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading all text files from the folders / ladataan kaikki tekstitiedostot kansiosta\n",
    "\n",
    "content = []\n",
    "docs_complete = []\n",
    "\n",
    "for paths, dirs, files in os.walk(notebookPath + \"/downloads/\"):\n",
    "    for file in files:\n",
    "        if file.endswith('.txt'):\n",
    "            with open(os.path.join(paths,file), 'r', encoding=\"utf-8\") as f:\n",
    "                if len(content) != 0:\n",
    "                    content.append(' ')\n",
    "                currentContent = f.read().lower().split(' ')\n",
    "                content += currentContent\n",
    "                docs_complete.append(currentContent)\n",
    "            \n",
    "print(\"sanojen määrä kaikki leikkeet kansiossa:\",len(content))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#poistetaan alle 2 merkin pituiset sanat\n",
    "content2=[stri2 for stri2 in content if len(stri2)>1]\n",
    "len(content2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#poistetaan ohitettavat sanat (stopwords)\n",
    "filtered_content3=[]\n",
    "for r in content2:\n",
    "    #print(r)\n",
    "    if not r in stop_words:\n",
    "        filtered_content3.append(r)\n",
    "len(filtered_content3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freqz3=nltk.FreqDist(content2)\n",
    "freqz3.most_common()[0:15]\n",
    "\n",
    "freqz4=nltk.FreqDist(filtered_content3)\n",
    "\n",
    "print(freqz3.most_common()[0:10])\n",
    "print(freqz4.most_common()[0:10])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WordClouds / sanapilvet (kaikki hakemistossa olevat sivut)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "content_str2 = ' '.join(content2)\n",
    "\n",
    "\n",
    "\n",
    "fig = plt.figure(figsize=(16, 10))\n",
    "plt.subplot(223)\n",
    "wordcloud3 = WordCloud(max_font_size=50,min_font_size=5).generate(content_str2)\n",
    "\n",
    "plt.imshow(wordcloud3, interpolation=\"bilinear\")\n",
    "plt.title(\"all \"+str(len(docs_complete))+\" documents, and \"+str(len(content2))+\" words, from folder\")\n",
    "\n",
    "plt.axis(\"off\")\n",
    "\n",
    "\n",
    "content_str3 = ' '.join(filtered_content3)\n",
    "\n",
    "plt.subplot(224)\n",
    "wordcloud4 = WordCloud(max_font_size=50,min_font_size=5).generate(content_str3)\n",
    "plt.imshow(wordcloud4, interpolation=\"bilinear\")\n",
    "\n",
    "plt.title(\"all \"+str(len(docs_complete))+\" documents, and \"+str(len(filtered_content3))+\" words, from folder (removed stopwords)\")\n",
    "\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Topic Modeling / Aihemallinnus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#based on \n",
    "#https://www.analyticsvidhya.com/blog/2016/08/beginners-guide-to-topic-modeling-in-python/\n",
    "\n",
    "len(docs_complete),len(content2),len(filtered_content3)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "import string\n",
    "stop = set(stopwords.words('finnish'))\n",
    "exclude = set(string.punctuation)\n",
    "lemma = WordNetLemmatizer()\n",
    "\n",
    "def clean(doc):\n",
    "    doc2=[word for word in doc if len(word)>3]       \n",
    "    stop_free = \" \".join([i for i in doc2 if i not in stop])\n",
    "    punc_free = ''.join(ch for ch in stop_free if ch not in exclude)\n",
    "    normalized = \" \".join(lemma.lemmatize(word) for word in punc_free.split())\n",
    "    return normalized\n",
    "\n",
    "    \n",
    "doc_clean = [clean(doc).split() for doc in docs_complete]        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#doc_clean=docs2#filtered_content2#map(unicode,filtered_content2)\n",
    "# Importing Gensim\n",
    "import gensim\n",
    "from gensim import corpora\n",
    "dictionary = corpora.Dictionary(doc_clean )\n",
    "#dictionary\n",
    "# Creating the term dictionary of our courpus, where every unique term is assigned an index. dictionary = corpora.Dictionary(doc_clean)\n",
    "\n",
    "# Converting list of documents (corpus) into Document Term Matrix using dictionary prepared above.\n",
    "doc_term_matrix = [dictionary.doc2bow(doc) for doc in doc_clean]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the object for LDA model using gensim library\n",
    "\n",
    "Lda = gensim.models.ldamodel.LdaModel\n",
    "\n",
    "# Running and Training LDA model on the document term matrix.\n",
    "ldamodel = Lda(doc_term_matrix, num_topics=100, id2word = dictionary, passes=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Results\n",
    "print(ldamodel.print_topics(num_topics=100, num_words=3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ldamodel.print_topics(num_topics=10, num_words=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(docs_complete),len(doc_clean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TOPIC MODELING2 https://de.dariah.eu/tatom/topic_model_python.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FILES\n",
    "FILES2=[]\n",
    "        \n",
    "for paths, dirs, files in os.walk(notebookPath + \"/downloads/\"):\n",
    "    for file in files:\n",
    "        if file.endswith('.txt'):\n",
    "            FILES2.append(os.path.join(paths,file))\n",
    "            print(FILES2[-1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############\n",
    "\n",
    "import numpy as np\n",
    "#FILES[0:4]\n",
    "stop = list(set(stopwords.words('finnish')))\n",
    "\n",
    "import sklearn.feature_extraction.text as text\n",
    "vectorizer = text.CountVectorizer(input='filename', stop_words=stop, min_df=1)\n",
    "dtm = vectorizer.fit_transform(FILES2).toarray()\n",
    "vocab = np.array(vectorizer.get_feature_names())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtm.shape,len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import decomposition\n",
    "num_topics = 10\n",
    "num_top_words = 10\n",
    "clf = decomposition.NMF(n_components=num_topics, random_state=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doctopic = clf.fit_transform(dtm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_words = []\n",
    "for topic in clf.components_:\n",
    "   word_idx = np.argsort(topic)[::-1][0:num_top_words]\n",
    "   topic_words.append([vocab[i] for i in word_idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doctopic = doctopic / np.sum(doctopic, axis=1, keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "novel_names = []\n",
    "\n",
    "# turn this into an array so we can use NumPy functions\n",
    "#novel_names = np.asarray(novel_names)\n",
    "novel_names=np.asarray(FILES2)\n",
    "doctopic_orig = doctopic.copy()\n",
    "\n",
    "# use method described in preprocessing section\n",
    "num_groups = len(set(novel_names))\n",
    "doctopic_grouped = np.zeros((num_groups, num_topics))\n",
    "\n",
    "for i, name in enumerate(sorted(set(novel_names))):\n",
    "        doctopic_grouped[i, :] = np.mean(doctopic[novel_names == name, :], axis=0)\n",
    "\n",
    "doctopic = doctopic_grouped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "novels = sorted(set(novel_names))\n",
    "\n",
    "print(\"Top NMF topics in...\")\n",
    "\n",
    "\n",
    "for i in range(len(doctopic)):\n",
    "        top_topics = np.argsort(doctopic[i,:])[::-1][0:3]\n",
    "        top_topics_str = ' '.join(str(t) for t in top_topics)\n",
    "        print(\"{}: {}\".format(novels[i], top_topics_str))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for t in range(len(topic_words)):\n",
    "   print(\"Topic {}: {}\".format(t, ' '.join(topic_words[t][:15])))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "nbTranslate": {
   "displayLangs": [
    "fi",
    "en"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "fi",
   "targetLang": "en",
   "useGoogleTranslate": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
